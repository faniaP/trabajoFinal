\documentclass[12pt,twocolumn,draft]{article} 

\usepackage{times}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{url,hyperref}
\usepackage{cite}
\usepackage{algorithm}
\usepackage{algorithmic}
\input{spanishAlgorithmic}


\begin{document}

\title{\textbf{Implementaci\'on de Risk con algoritmo de Minimax.}}
\author{Prieto, Estefan\'ia$^{[1]}$\\
Galicia, Fernando$^{[1]}$\\
Galv\'an, Antonio$^{[1]}$\\
$^{[1]}$Facultad de Ciencias, U.N.A.M.\\
\\
estefaniaprieto@ciencias.unam.mx\\
fernandogamen@ciencias.unam.mx\\
g.antonio@ciencias.unam.mx\\
\date{21-Noviembre-2014}
}

\twocolumn[\begin{@twocolumnfalse}
\maketitle
\thispagestyle{empty}
\begin{abstract}
A diferencia de los juegos de apuesta, donde el jugador se pregunta \textit{"?`Cu\'al es la mejor jugada para ganar un juego?"} y as\'i poder ser el due\~{n}o de un premio (generalmente un incentivo monetario), es bien sabido en la teor\'ia de juegos la motiva escenarios tales c\'omo el ajedrez, go, gato, etc. No existe tal pregunta, si no, \'esta se replantea una expresi\'on de la forma \textit{"?`Existe una mejor forma de jugar en tal escenario?"}.\\
Por lo cu\'al se propone un modelo de Inteligencia Artificial para una versi\'on acotada del juego \textbf{Risk} basado en minimax, con base en estrategias muy complicadas implementadas por un experto, hasta muy b\'asicas diseñadas por un novato en el juego.\\
\end{abstract}
\end{@twocolumnfalse}]


\section{Introducci\'on.}

Hacer ac\'a la introducci\'on de minimax.


\section{Juegos con informaci\'on perfecta. [MODIFICAR A INFORMARCI\'ON IM-PERFECTA Y ARGUMENTAR EL POR QUE PODEMOS ADAPTARLO AS\'I.]}
Explicar por que catalogamos al \textbf{RISK} c\'omo un juego de informaci\'on perfecta y por que hemos elegido esta implementaci\'on.


% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% % % % % % % % % % % % RISK ACOTADO % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
\section{Risk acotado.}

Tal y c\'omo se plantea en el juego original (\textbf{\textit{ve\'ase \cite{RISK}}}) el objetivo del juego continua siendo la dominaci\'on total de un territorio dado, de tal forma
que el juego queda concluido cu\'ando todos los territorios quedan bajo la dominaci\'on de 
un jugador.\\
En esta implementaci\'on acotaremos la cantidad de continentes, es decir, el desarrollo sera unicamente en un solo continente, tambi\'en la cantidad de dados se ve acotada a unicamente dos dados y restringido a dos jugadores.\\

Sin embargo mantendremos las dem\'as condiciones iniciales con respecto a las tropas y al equivalente de tropas en cada territorio, es decir:
\begin{list}{*}{}
\item Cada unidad representa una \textit{Armada}.
\item Cada \textit{Caballer\'ia} representa 5 unidades.
\item Cada \textit{Artiller\'ia} representa 10 unidades.
\end{list}

Teniendo ya esto definido, entonces, cada jugador tendr\'a un ejercito inicial de 35 tropas.

\section{Descripci\'on del agente.}
Aqu\'i es donde describimos el comportamiento del agente.

\section{Generaci\'on de movimientos}

\subsection{Reforzamientos}

Es m\'as f\'acil comenzar aqu\'i, ya que \'unicamente hay que tomar los v\'ertices de cada jugador
y aumentar el n\'umero de tropas en 2.\\

Para lograr esto, por cada nodo existir\'a una nueva gr\'afica con el nuevo n\'umero de tropas, lo
cual significa que habr\'a tantas nuevas gr\'aficas como paises pertenezcan a cada jugador; para
esto utilizaremos \textit{BFS}.

\begin{algorithm}
\begin{algorithmic}[1]
\REQUIRE Gr\'afica $G$ que representa el tablero actual,Jugador $actual$
\ENSURE Lista con todos los posibles reforzamientos
\STATE{$Queue$ $q$}
\STATE{$List reforzamientos$}
\FORALL{$v \in G$}
\STATE{$v.visitado = FALSE$}
\ENDFOR
\STATE{$v_{1}.visitado = FALSE$}\COMMENT{$v_{1}$ es el pa\'is con identificador 1}
\STATE{$v_{1}.tropas = v_{1}.tropas+2$}
\STATE{$reforzamientos.add(G)$}
\STATE{$q.enqeue(v_{1})$}
\WHILE{$q.isNotEmpty()$}
\STATE{$v = q.remove()$}
\FORALL{$u \in Vecinos(v)$}
\IF{$u.visitado = FALSE$}
\STATE{$u.tropas = u.tropas+2$}
\STATE{$reforzamientos.add(G)$}
\STATE{$u.visitado = TRUE$}
\STATE{$q.enqeue(u)$}
\ENDIF
\ENDFOR
\ENDWHILE
\RETURN $reforzamientos$
\end{algorithmic}
\caption{Definici\'on de la funci\'on $reforzamientos$}
\label{reforzamientos}
\end{algorithm}

\section{Funci\'on de evaluaci\'on.}

Dado que el juego de risk consiste en dos partes, entonces se necesitan dos series de estrategias: la
primera para la invasi\'on de territorio y la segunda parte consiste en reforzamiento y ataque;
por lo que se necesitan dos funciones de evaluaci\'on.

\subsection{Invasi\'on}

La estrateg\'ia consiste en invadir los paises con mayor n\'umero de vecinos y poder encapsular al enemigo.

$$ 
F(n)= \left\{\begin{array}{c l}
  \infty\ \textit{Max\ obtiene\ los\ paises\ con\ mayor\ grado.}\\
  -\infty\ \textit{Min\ obtiene\ los\ paises\ con\ mayor\ grado.}\\
  \mathtt{invasion(n)}\ \textit{e.o.c.}
\end{array}
\right.
$$

Donde:\\

$\mathtt{invasion}$ por medio de \textbf{BFS} suma al valor de la funci\'on de evaluaci\'on: el
grado de cada v\'ertice, el total de vecinos no ocupados por ninguno de los jugadores; y por \'ultimo
resta los vecinos ocupados por \textbf{min} por cada pa\'is de \textbf{MAX}.\\

\begin{algorithm}
\begin{algorithmic}[1]
\REQUIRE La gr\'afica que representa el tablero
\ENSURE Es el entero descrito anteriormente
\STATE{$puntuacion = 0$}
\STATE{$Queue$ $q$}
\FORALL{$v \in G$}
\STATE{$v.visitado = FALSE$}
\ENDFOR
\STATE{$v_{1}.visitado = FALSE$}\COMMENT{$v_{1}$ es el pa\'is con identificador 1}
\STATE{$puntuacion = v_{1}.grado$}
\STATE{$q.enqeue(v_{1})$}
\WHILE{$q.isNotEmpty()$}
\STATE{$v = q.remove()$}
\FORALL{$u \in Vecinos(v)$}
\IF{$u.visitado = FALSE$}
\IF{$puntuacion < u.grado$}
\STATE{$puntuacion = u.grado$}
\ENDIF
\IF{$v.Jugador = MAX\ and\ Jugador = MIN$}
\STATE{$puntuacion = puntuacion - 1$}
\ENDIF
\STATE{$u.visitado = TRUE$}
\STATE{$q.enqeue(u)$}
\ENDIF
\ENDFOR
\ENDWHILE
\RETURN $puntuacion$
\end{algorithmic}
\caption{Definici\'on de la funci\'on $inavsion$}
\label{invasion}
\end{algorithm}

\subsection{Reforzamiento y ataque}

La estrateg\'ia consiste en tomar cada pa\'is ocupado por \textbf{MAX} y ver a todos sus vecinos,
se realiza una comparac\'ion de que paises son del oponente y cuanto se diferencian las tropas, para así tomar su
mejor desici\'on de ataque o reforzamiento.

$$ 
F(n)= \left\{\begin{array}{c l}
  \infty\ \textit{Max\ resulta\ ser\ ganador.}\\
  -\infty\ \textit{Min\ resulta\ ser\ ganador.}\\
  \mathtt{reforzaAtaca(n)}\ \textit{e.o.c.}
\end{array}
\right.
$$

Donde:\\

$\mathtt{reforzaAtaca}$ es un algoritmo que por medio de una modificaici\'on a \textit{BFS} cuenta
las tropas y paises de cada jugador, suma los pertenecientes al jugador \textbf{MAX} y resta los del oponente 
\textbf{min}, tambi\'en por cada pa\'is de \textbf{MAX} resta los vecinos que pertenezcan a \textbf{min}, por otra
parte bajo esa misma idea compara el n\'umero de tropas.\\



\begin{algorithm}
\begin{algorithmic}[1]
\REQUIRE La gr\'afica que representa el tablero
\ENSURE Es el entero descrito anteriormente
\STATE{$puntuacion = 0$}
\STATE{$Queue$ $q$}
\FORALL{$v \in G$}
\STATE{$v.visitado = FALSE$}
\ENDFOR
\COMMENT{$v_{1}$ es el pa\'is con identificador 1}
\STATE{$v_{1}.visitado = FALSE$}
\IF{$v_{1}.jugador = MAX$}
\STATE{$puntuacion = puntuacion + u.daTropas() + 1$}
\ENDIF
\IF{$v_{1}.jugador = MIN$}
\STATE{$puntuacion = puntuacion - u.daTropas() - 1$}
\ENDIF
\STATE{$q.enqeue(v_{1})$}
\WHILE{$q.isNotEmpty()$}
\STATE{$v = q.remove()$}
\FORALL{$u \in Vecinos(v)$}
\IF{$u.visitado = FALSE$}
\IF{$u.jugador = MAX$}
\STATE{$puntuacion = puntuacion + u.daTropas() + 1$}
\ENDIF
\IF{$u.jugador = MIN$}
\STATE{$puntuacion = puntuacion - u.daTropas() - 1$}
\ENDIF
\IF{$v.Jugador = MAX\ and\ u.Jugador = MIN$}
\STATE{$puntuacion = puntuacion - 1$}
\ENDIF
\IF{$v.tropas > u.tropas$}
\STATE{$puntuacion = puntuacion + 1$}
\ENDIF
\IF{$v.tropas < u.tropas$}
\STATE{$puntuacion = puntuacion - 1$}
\ENDIF
\STATE{$u.visitado = TRUE$}
\STATE{$q.enqeue(u)$}
\ENDIF
\ENDFOR
\ENDWHILE
\RETURN $puntuacion$
\end{algorithmic}
\caption{Definici\'on de la funci\'on $reforzaAtaca$}
\label{reforzaAtaca}
\end{algorithm}




\section{Minimax}

Es un algoritmo para \textit{minimizar} la p\'erdida \textit{m\'axima} esperada en juegos de adversarios con
informaci\'on perfecta.\\

Como se mencion\'o anteriormente dado que el juego de \textbf{Risk} es pr\'acticamente intratable, se pierde esta propiedad
de informaci\'on perfecta, ya que el factor de ramificaci\'on es demasiado grande para poder ser implementada.\\

La idea te\'orica del algoritmo minimax es generar todo el \'arbol del juego, asignarles valor a cada
nodo del \'arbol y hacer un recorrido \textit{DFS} para obtener la mejor estrateg\'ia para \textbf{MAX}.\\

Dado que esto requiere una gran cantidad de espacio y tiempo, entonces la pr\'actica usual es realizar
el algoritmo \textit{minimax} de forma recursiva, tal que, vaya simulando la creaci\'on de las ramas y despu\'es
asginarles su valor y por \'ultimo obtener la mejor estrateg\'ia.\\

\begin{algorithm}
\begin{algorithmic}[1]
\REQUIRE Entero $p$ que representa la profundiad del \'arbol, Jugador $actual$, gr\'afica $G$ que representa el tablero actual,
funcion de evaluacion $f$
\ENSURE  Gr\'afica que representa la mejor jugada
\STATE{$List\ movimientos$}
\STATE{$mejorPuntuacion = 0$}
\IF{$actual = 1$}
\STATE{$mejorPuntuacion = -\infty$}
\ELSE
\STATE{$mejorPuntuacion = \infty$}
\ENDIF
\STATE{$puntuacionActual = 0$}
\STATE{$mejorMovimiento = null$}
\IF{$movimientos.isEmpty()\ or\ profundidad = 0$}
\STATE{$mejorPuntuacion = f(G)$}
\STATE{$mejor = G$}
\ELSE
\FORALL{$movimiento \in movimientos$}
\IF{$actual = 1$}
\STATE{$puntuacionActual = f(minimax(p-1,min,G,f))$}
\IF{$puntuacionActual > mejorPuntuacion$}
\STATE{$mejorPuntuacion = puntuacionActual$}
\STATE{$mejor = movimiento$}
\ENDIF
\ELSE
\STATE{$puntuacionActual = f(minimax(p-1,max,G,f))$}
\IF{$puntuacionActual < mejorPuntuacion$}
\STATE{$mejorPuntuacion = puntuacionActual$}
\STATE{$mejor = movimiento$}
\ENDIF
\ENDIF
\ENDFOR
\ENDIF
\RETURN{$mejor$}
\end{algorithmic}
\caption{Definici\'on de $minimax$}
\label{minimax}
\end{algorithm}



\section{Especificaciones del programa.}

La implementaci\'on concreta del proyecto se ha realizado en el lenguaje de programaci\'on \textbf{\textit{Java}}\cite{JAVA} que se ha optado por que \textit{Escribir ventajas de java y por que hemos optado por \'el}\\

Una vez aclarado esto, introduciremos la representaci\'on del territorio por medio de un archivo llamado \textit{"Territorio.xml"} en el cual obtenemos las ventajas de que \'este nos brinda una estructura la cual nos permite adaptar informaci\'on de manera independiente al manejo de \'esta \cite{xml}. As\'i el territorio del juego en el que cada pa\'is tendr\'a una etiqueta que lo represente y dentro de \'esta estar\'an especificados los atributos de cada pa\'is.\\

De esta, hemos seccionado a los pa\'ises en una peque\~na base de datos. As\'i que usaremos la interfaz \textit{JAXP}\cite{JAXP}

\section{Conclusiones.}
Informar las conclusiones que hemos encontrado en nuestra implementaci\'on.\\


% %Toda la bibliografía consultada debe de estar anexada en el archivo
% %referencias.bib 
\newpage
\bibliographystyle{plain}	
\bibliography{referencias.bib}{}

\end{document}
